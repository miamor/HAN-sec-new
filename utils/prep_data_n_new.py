"""
prep_data__doc2vec_edge_node
"""
import sys
# sys.path.insert(1, '../..')
from .utils import indices_to_one_hot, label_encode_onehot, sample_mask, preprocess_adj, preprocess_features, save_pickle, load_pickle, save_txt, care_APIs

import os
import json
import numpy as np
import scipy.sparse as sp
from dgl import DGLGraph
import math
import random
from utils.constants import *
import shutil

import torch
import torch.nn as nn

from graphviz import Digraph
import pydot
import networkx as nx
import matplotlib.pyplot as plt
from utils.word_embedding import TFIDF, Doc2Vec_

from multiprocessing.pool import ThreadPool
from concurrent.futures.thread import ThreadPoolExecutor
from multiprocessing import Process

__THREADS_NUM__ = 20

class PrepareData(object):
    DATA_OUT_PATH = ''  # data root dir
    # folder contains pickle file (save data in dortmund format)
    pickle_folder = ''
    from_folder = False

    reports_parent_dir_path = ''  # path to report.json generated by cuckoo
    # consider only apis that fall into these categories
    # allow_cat = ['network', 'file', 'registry', 'process']
    allow_cat = ['file', 'process', 'registry']

    use_interesting_apis = None
    mapping_labels = {'benign': 0, 'malware': 1}

    path_type_code = {
        'proc__process_api': 0,
        'proc__reg_api': 1,
        'proc__file_api': 2,
        'file__file_api': 3,
        'reg__reg_api': 4,
        'proc__other_api': 5
    }
    node_type_code = {
        'proc': 0, # process_handle
        'file': 1, # file_handle
        'reg': 2, # registry key_handle

        'process_api': 3,
        'file_api': 4,
        'reg_api': 5,

        'other_api': 4,
    }
    node_color = ['red', 'orange', 'blue', 'pink', 'yellow', 'cyan']

    interesting_apis = care_APIs() + list(node_type_code.keys()) + ['other']


    def __init__(self, config, cuckoo_analysis_dir):
        self.cuckoo_analysis_dir = cuckoo_analysis_dir
        # self.vocab_path = vocab_path.split('.txt')[0]
        self.vocab_path = config['vocab_path']
        # self.vocab_path = vocab_path
        self.vocab_path_node = self.vocab_path+'/node.txt'
        self.vocab_path_edge = self.vocab_path+'/edge.txt'
        
        self.save_json = True
        
        self.do_draw = config['do_draw'] if 'do_draw' in config else False
        self.partition = config['partition'] if 'partition' in config else False

        self.preprocess_level = config['preprocess_level']

        self.reset()
        
        self.use_interesting_apis = config['use_interesting_apis']
        self.prepend_vocab = config['prepend_vocab']
        self.mapping_path = config['mapping_path']

        print('[__init__] self.partition', self.partition)
        print('[__init__] self.prepend_vocab', self.prepend_vocab)
        print('[__init__] self.use_interesting_apis', self.use_interesting_apis)
        print('[__init__] self.mapping_path', self.mapping_path)

        if config['input_report_folder'] is not None:
            self.reports_parent_dir_path = config['input_report_folder']

        if config['input_data_folder'] is not None:
            self.final_json_path = config['input_data_folder']
            data_dir = os.path.dirname(self.final_json_path)
            if not os.path.isdir(data_dir):
                os.makedirs(data_dir)
                
            json_data_dir = self.final_json_path
            if not os.path.isdir(json_data_dir):
                os.makedirs(json_data_dir)
            for key in self.json_data_paths:
                self.json_data_paths[key] = json_data_dir+'/'+key+'.json'


        if config['input_pickle_folder'] is not None:
            self.pickle_folder = config['input_pickle_folder']
            print('[__init__] self.pickle_folder', self.pickle_folder)
            if not os.path.isdir(self.pickle_folder):
                os.makedirs(self.pickle_folder)
            # copy config file to this
            shutil.copy(config['config_fpath'], self.pickle_folder)
            
            self.graph_folder = self.pickle_folder.replace('data_pickle', 'data_graphs')
            print('[__init__] self.graph_folder', self.graph_folder)
            if not os.path.isdir(self.graph_folder):
                os.makedirs(self.graph_folder)

            self.graph_viz_dir = self.pickle_folder.replace('data_pickle', 'data_graphviz')
            print('[__init__] self.graph_viz_dir', self.graph_viz_dir)
            if not os.path.isdir(self.graph_viz_dir):
                os.makedirs(self.graph_viz_dir)


        if self.mapping_path is not None:
            with open(self.mapping_path) as json_file:
                self.mapping = json.load(json_file)
                for cls_name in self.mapping:
                    self.edge_args_embedding_data_csv__cls[cls_name] = []
                    self.node_name_embedding_data_csv__cls[cls_name] = []

            with open(self.pickle_folder+'/mapping.json', 'w') as f:
                json.dump(self.mapping, f)


        self.from_folder = config['from_folder']
        self.from_json = config['from_json']
        self.from_pickle = config['from_pickle']
        self.from_each_graph_pickle = config['from_each_graph_pickle'] if 'from_each_graph_pickle' in config else False
        print('self.from_folder', self.from_folder, 'self.from_json', self.from_json, 'self.from_pickle', self.from_pickle)


        self.reverse_edge = config['reverse_edge']

        self.train_embedder = config['train_embedder']

        self.edge_args_emb_trained_path = config['train_embedding_path']+'/edge_csv'
        if not os.path.exists(self.edge_args_emb_trained_path):
            os.makedirs(self.edge_args_emb_trained_path)
        self.edge_args_emb_corpus_path = self.pickle_folder+'/edge_csv'
        if not os.path.exists(self.edge_args_emb_corpus_path):
            os.makedirs(self.edge_args_emb_corpus_path)

        self.node_name_emb_trained_path = config['train_embedding_path']+'/node_csv'
        if not os.path.exists(self.node_name_emb_trained_path):
            os.makedirs(self.node_name_emb_trained_path)
        self.node_name_emb_corpus_path = self.pickle_folder+'/node_csv'
        if not os.path.exists(self.node_name_emb_corpus_path):
            os.makedirs(self.node_name_emb_corpus_path)


        print('self.edge_args_emb_trained_path', self.edge_args_emb_trained_path)
        print('self.edge_args_emb_corpus_path', self.edge_args_emb_corpus_path)


        max_ft = config['max_ft']
        top_k = config['top_k']
        vector_size = config['vector_size']
        dm = config['dm']

        self.prepare_word_embedding = config['prepare_word_embedding']
        # if self.train_embedder is False:
        #     self.prepare_word_embedding = False

        if 'train_list_file' in config and os.path.exists(config['train_list_file']):
            with open(config['train_list_file'], 'r') as f:
                lines = f.read().split('\n')
                self.train_list_name = [line.strip() for line in lines]
            with open(config['test_list_file'], 'r') as f:
                lines = f.read().split('\n')
                self.test_list_name = [line.strip() for line in lines]

        if config['edge_embedder'] == 'doc2vec':
            self.edge_embedder = Doc2Vec_(self.edge_args_emb_trained_path, self.edge_args_emb_corpus_path, self.mapping, vector_size, dm)
        elif config['edge_embedder'] == 'tfidf':
            self.edge_embedder = TFIDF(self.edge_args_emb_trained_path, self.edge_args_emb_corpus_path, self.mapping, max_ft, top_k)

        if config['node_embedder'] == 'doc2vec':
            self.node_embedder = Doc2Vec_(self.node_name_emb_trained_path, self.node_name_emb_corpus_path, self.mapping, vector_size, dm)
        elif config['node_embedder'] == 'tfidf':
            self.node_embedder = TFIDF(self.node_name_emb_trained_path, self.node_name_emb_corpus_path, self.mapping, max_ft, top_k)

        return


    def reset(self):
        print('============== RESET ==============')
        self.behavior = {}  # behavior extracted from report.json in json format

        self.final_json_data = {
            'nodes': [],
            'paths': []
        }
        self.final_json_path = ''  # path to data.json generated by process_txt() function
        
        self.json_data_paths = {
            'nodes': '',
            'proc__process_api': '',
            'proc__file_api': '',
            'proc__reg_api': '',
            'file__file_api': '',
            'reg__reg_api': '',
            'proc__other_api': ''
        }
        self.json_data = {
            'nodes': {},
            'proc__process_api': {},
            'proc__file_api': {},
            'proc__reg_api': {},
            'file__file_api': {},
            'reg__reg_api': {},
            'proc__other_api': {}
        }
        

        self.all_nodes = []

        self.edges = None
        self.nodes_labels = []
        self.edges_labels = []
        self.graphs_labels = []

        self.graphs_dict = dict()
        self.graphs_name_to_label = dict()
        self.graphs = []
        self.graphs_names = []
        self.graphs_viz = dict()

        self.word_dict_node = []
        self.word_dict_edge = []

        self.current_edge_id = -1
        self.current_node_id = -1
        self.current_node_id_of_current_graph = -1
        self.current_edge_id_of_current_graph = -1
        self.api_nodes_existed_id = {}
        self.api_nodes_existed = {}
        # create_apis = ['open', 'create', 'set', 'write']

        self.handle_to_pid = {}
        self.handle_to_node = {}
        self.pid_to_node = {}

        self.data_dortmund_format = {}
        self.max_n_nodes = 0

        self.word_to_ix_node = {}
        self.word_to_ix_edge = {}

        self.edge_args_embedding_data_csv = []
        self.edge_args_embedding_data_csv__cls = {}
        self.node_name_embedding_data_csv = []
        self.node_name_embedding_data_csv__cls = {}
        self.edge_args_by_graph = {}
        self.node_names_by_graph = {}
        self.flags_keys = []
        self.mapping = {}

        self.g_names = []

        self.node_name_code = { self.interesting_apis[i]: i for i in range(0, len(self.interesting_apis)) }


    def load_data(self):
        """
        Return self.data_dortmund_format
        """
        self.has_label = True
        if self.from_pickle is True:
            print('\n[load_data] Train & load node/edge embedding')
            # self.train_and_load_embedding()

            print('[load_data] Load data from pickle folder')
            self.load_from_pickle()
            return self.data_dortmund_format

        if self.partition is False or self.from_each_graph_pickle is False:
            ''' Copy prep_data file to this data folder '''
            if self.final_json_path is not None:
                shutil.copy('./utils/prep_data.py', self.final_json_path+'/../prep_data.py')

            if self.from_folder is True:
                if self.from_json is False:
                    self.save_json = True
                self.prepare_word_embedding = True
                
                if self.from_json is True:
                    print('\n[load_data] Load data from json file')
                    self.load_data_json_multithread()

                print('\n[load_data] Process data from reports folder to data.json and encode nodes & edges')
                self.encode_reports_from_dir(self.save_json)

                print('\n[load_data] Gen corpus')
                self.gen_vocab_and_corpus()

            elif self.from_json is True:
                print('\n[load_data] Load data from json file')
                self.load_data_json_multithread()


            print('\n[load_data] Train & load node/edge embedding')
            self.train_and_load_embedding()

            if self.from_json is True:
                # if self.partition is False:
                print('\n[load_data] Encode nodes & edges')
                self.encode_data()
            
        # Use when handling with large dataset like pack1 (run in partition) 
        # if self.from_json is True and (self.partition is True or self.do_draw is True):
        #     if self.from_json is True:
        if self.partition is True:
            num_g = None
            tot_g = 0
            if self.from_each_graph_pickle is False:
                print('\n[load_data] Encode nodes & edges and create seperate graphs')
                self.finish_graphed = False
                # num_g = -1
                for report_dir_name in sorted(os.listdir(self.reports_parent_dir_path)): # category name (benign/malware)
                    report_dir_path = os.path.join(self.reports_parent_dir_path, report_dir_name)
                    # for report_file_name in sorted(os.listdir(report_dir_path)):
                    #     g_name = '{}__{}'.format(report_dir_name, report_file_name)
                    #     self.create_graph_from_report(g_name, report_dir_name)

                    # Multiprocessing (error)
                    # report_file_paths = [os.path.join(report_dir_path, report_file_name) for report_file_name in sorted(os.listdir(report_dir_path))]
                    # tot_g += len(report_file_paths)
                    # results = ThreadPool(__THREADS_NUM__).imap_unordered(self.create_graph_from_report_multithread, sorted(report_file_paths))
                    # num_g = len(results)

                    # Multi threading
                    # num_g = 0
                    with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
                        for report_file_name in sorted(os.listdir(report_dir_path)):
                            g_name = '{}__{}'.format(report_dir_name, report_file_name)
                            executor.submit(self.create_graph_from_report, g_name, report_dir_name)

            if num_g is None or num_g == tot_g:
                print('\n[load_data] Merge graphs and save to pickle files from encoded data')
                self.merge_graphs()
        else:
            print('\n[load_data] Create graphs and save to pickle files from encoded data')
            self.create_graphs()

        return self.data_dortmund_format


    def load_data_files_extension(self, report_path, report_dir_name, report_file_name):
        print('[load_data_files_extension] report_file_name', report_dir_name, report_file_name)
        print('[load_data_files_extension] report_path', report_path)
        behavior = self.read_report(report_path)

        if behavior is not None:
            self.encode_report(behavior, report_file_name, report_dir_name)
        else:
            print('[load_data_files_extension] behavior none. Skip '+report_dir_name+'/'+report_file_name)

    def load_data_files(self, task_ids, report_file_name=None, report_dir_name=None, report_dir_path=None, write_to_json=True):
        """
        Preprocess data from cuckoo report
        report_dir_name = str(task_id)
        Return self.data_dortmund_format
        """
        # overwrite graphviz dir for tasks
        self.reports_parent_dir_path = ''
        if report_dir_path is None:
            report_dir_path = 'api_tasks/data_report'

        self.has_label = False

        if self.from_pickle is True:
            print('\n[load_data_files] Train & load node/edge embedding')
            # self.train_and_load_embedding()

            print('[load_data_files] Load data from pickle folder')
            self.load_from_pickle()
            return self.data_dortmund_format

        print('\n[load_data_files] Process data from one or some report files to json format and encode nodes & edges')
        if task_ids is None:            
            # self.graph_viz_dir = 'api_tasks/graphviz/'+report_dir_name
            # if not os.path.exists(self.graph_viz_dir):
            #     os.makedirs(self.graph_viz_dir)

            if report_file_name is None: # loop through folder
                # for report_file_name in sorted(os.listdir(report_dir_path+'/'+report_dir_name)):
                #     report_path = report_dir_path+'/'+report_dir_name+'/'+report_file_name
                #     self.load_data_files_extension(report_path, report_dir_name, report_file_name)
                with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
                    for report_file_name in sorted(os.listdir(report_dir_path+'/'+report_dir_name)):
                        report_path = report_dir_path+'/'+report_dir_name+'/'+report_file_name
                        executor.submit(self.load_data_files_extension, report_path, report_dir_name, report_file_name)

            else: # process 1 file only
                report_path = report_dir_path+'/'+report_dir_name+'/'+report_file_name
                self.load_data_files_extension(report_path, report_dir_name, report_file_name)

        else:
            report_file_name = 'report.json'
            # for task_id in task_ids:
            #     report_dir_name = str(task_id)
            #     report_path = '{}/{}/reports/{}'.format(self.cuckoo_analysis_dir, report_dir_name, report_file_name)

            #     # self.graph_viz_dir = 'api_tasks/graphviz/'+report_dir_name
            #     # if not os.path.exists(self.graph_viz_dir):
            #     #     os.makedirs(self.graph_viz_dir)

            #     self.load_data_files_extension(report_path, report_dir_name, report_file_name)
            with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
                for task_id in task_ids:
                    report_dir_name = str(task_id)
                    report_path = '{}/{}/reports/{}'.format(self.cuckoo_analysis_dir, report_dir_name, report_file_name)

                    # self.graph_viz_dir = 'api_tasks/graphviz/'+report_dir_name
                    # if not os.path.exists(self.graph_viz_dir):
                    #     os.makedirs(self.graph_viz_dir)

                    self.load_data_files_extension(report_path, report_dir_name, report_file_name)
                    executor.submit(self.load_data_files_extension, report_path, report_dir_name, report_file_name)


        self.gen_edge_args_embedding_data()
        self.gen_node_name_embedding_data()

        ''' Save processed json '''
        if write_to_json is True:
            print('[load_data_files] Process done. Saving to json file...')
            # Save json data to file
            # it's gonna be too large, use separate file to store information as belows
            n_empty_obj = 0
            for key in self.json_data_paths:
                with open(self.json_data_paths[key], 'w') as outfile:
                    json.dump(self.json_data[key], outfile)
                    if bool(self.json_data[key]) is False:
                        n_empty_obj += 1
            if n_empty_obj == len(self.json_data):
                print('[load_data_files] Graph cant be created')
                return None
        else:
            print('[load_data_files] Writing to json file option set to False. Skip saving.')

        print('\n[load_data_files] Train & load node/edge embedding')
        self.train_embedder = False
        self.from_folder = False
        self.train_and_load_embedding()

        print('\n[load_data_files] Encode nodes & edges')
        # self.encode_data()

        # print('\n[load_data_files] Create graphs and save to pickle files from encoded data')
        # self.create_graphs()


        with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
            for report_file_name in sorted(os.listdir(report_dir_path+'/'+report_dir_name)):
                g_name = '{}__{}'.format(report_dir_name, report_file_name)
                executor.submit(self.create_graph_from_report, g_name, '')

        print('\n[load_data] Merge graphs and save to pickle files from encoded data')
        self.merge_graphs()

        return self.data_dortmund_format


    def encode_reports_from_dir_multithread(self, report_file_path):
        report_file_name = os.path.basename(report_file_path)
        report_dir_name = os.path.basename(os.path.dirname(report_file_path))
        
        print('[encode_reports_from_dir]', report_dir_name+'/'+report_file_name)
        behavior = self.read_report(report_file_path)

        if behavior is not None:
            g_name = report_dir_name+'__'+report_file_name
            if g_name not in self.g_names:
                # print('\t [encode_reports_from_dir] Run encode_report()')
                self.encode_report(behavior, report_file_name, report_dir_name)
        else:
            print('[encode_reports_from_dir] behavior none. Skip ' +report_dir_name+'/'+report_file_name)

    def encode_reports_from_dir(self, write_to_json=True):
        for report_dir_name in os.listdir(self.reports_parent_dir_path):
            report_dir_path = os.path.join(self.reports_parent_dir_path, report_dir_name)
            # n = 0
            # for report_file_name in sorted(os.listdir(report_dir_path)):
            #     n += 1
            #     print('[encode_reports_from_dir]', n, report_dir_name+'/'+report_file_name)
            #     behavior = self.read_report(os.path.join(report_dir_path, report_file_name))

            #     if behavior is not None:
            #         self.encode_report(
            #             behavior, report_file_name, report_dir_name)
            #     else:
            #         print('[encode_reports_from_dir] behavior none. Skip ' +
            #               report_dir_name+'/'+report_file_name)

            report_file_paths = [os.path.join(report_dir_path, report_file_name) for report_file_name in sorted(os.listdir(report_dir_path))]
            ThreadPool(__THREADS_NUM__).imap_unordered(self.encode_reports_from_dir_multithread, sorted(report_file_paths))
            # with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
            #     for report_file_path in sorted(report_file_paths):
            #         executor.submit(self.encode_reports_from_dir_multithread, report_file_path)

        ''' Save processed json '''
        if write_to_json is True:
            print('[encode_reports_from_dir] Process done. Saving to json file...')
            # Save json data to file
            # it's gonna be too large, use separate file to store information as belows
            # for key in self.json_data_paths:
            #     with open(self.json_data_paths[key], 'w') as outfile:
            #         json.dump(self.json_data[key], outfile)
            ThreadPool(__THREADS_NUM__).imap_unordered(self.dump_one_json, list(self.json_data_paths.keys()))
            # with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
            #     for key in list(self.json_data_paths.keys()):
            #         executor.submit(self.dump_one_json, key)
        else:
            print('[encode_reports_from_dir] Writing to json file option set to False. Skip saving.')

        self.gen_edge_args_embedding_data()
        self.gen_node_name_embedding_data()

    def dump_one_json(self, key):
        with open(self.json_data_paths[key], 'w') as outfile:
            json.dump(self.json_data[key], outfile)


    def read_report(self, report_file_path):
        # print('[read_report] report_file_path', report_file_path)
        with open(report_file_path) as json_file:
            data = json.load(json_file)
            if 'behavior' in data.keys():
                return data['behavior']
            else:
                print('[read_report] No behavior tag found.')
                return None
            # self.behavior = data['behavior']

    def load_data_json(self):
        for key in self.json_data_paths:
            with open(self.json_data_paths[key]) as json_file:
                print('[load_data_json] Load '+self.json_data_paths[key])
                self.json_data[key] = json.load(json_file)
    
    
    def load_data_json_multithread(self):
        # results = ThreadPool(__THREADS_NUM__).imap_unordered(self.load_one_json, list(self.json_data_paths.keys()))
        with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
            for key in list(self.json_data_paths.keys()):
                executor.submit(self.load_one_json, key)
        self.g_names = []
        for node_id in self.json_data['nodes']:
            self.g_names.append(self.json_data['nodes'][node_id]['graph'])

    def load_one_json(self, key):
        if os.path.exists(self.json_data_paths[key]):
            with open(self.json_data_paths[key]) as json_file:
                print('[load_data_json] Load '+self.json_data_paths[key])
                self.json_data[key] = json.load(json_file)


    def add_edge_args_embedding_data(self, flags, label, graph_name):
        flags_data = []
        # print('flags', flags)
        for flag_key in sorted(flags):
            # print('flag_key', flag_key)
            if flag_key not in self.flags_keys:
                self.flags_keys.append(flag_key)

            if len(flags[flag_key]) > 0:
                if isinstance(flags[flag_key], list):
                    # print('flags', flags, 'flag_key', flag_key, 'flags[flag_key]', flags[flag_key])
                    
                    flag_data = flag_key
                    append = False
                    for v in flags[flag_key]:
                        # print('\t v', v)
                        if v is not None:
                            flag_data += ' ' + v.replace('|', ' ').lower()
                            append = True
                    if append is True:
                        flags_data.append(flag_data)
                else:
                    # flag_data = flags[flag_key].replace('|', ' ').lower()
                    flag_data = flag_key + ' ' + flags[flag_key].replace('|', ' ').lower()
                    flags_data.append(flag_data)
        
        if len(flags_data) > 0:
            if graph_name not in self.edge_args_by_graph:
                self.edge_args_by_graph[graph_name] = flags_data
            else:
                self.edge_args_by_graph[graph_name] += flags_data

    def gen_edge_args_embedding_data(self):
        print('[gen_edge_args_embedding_data] self.edge_args_by_graph', len(self.edge_args_by_graph))
        for graph_name in self.edge_args_by_graph:
            label = graph_name.split('__')[0]
            flags_data_txt = ' '.join(self.edge_args_by_graph[graph_name])
            print('[gen_edge_args_embedding_data] graph_name', graph_name)
            # print('[gen_edge_args_embedding_data] label', label)
            # print('[gen_edge_args_embedding_data] self.mapping', self.mapping)
            if self.has_label is True:
                self.edge_args_embedding_data_csv.append({'class': self.mapping[label], 'data': flags_data_txt, 'file': graph_name})
                # self.edge_args_embedding_data_csv__cls[label].append('{} {}'.format(flag_key, flags_data_txt))
                # self.edge_args_embedding_data_csv__cls[label].append({'class': self.mapping[label], 'data': flags_data_txt, 'file': graph_name})
            else:
                self.edge_args_embedding_data_csv.append({'class': -1, 'data': flags_data_txt, 'file': graph_name})
    # def load_edge_args_embedding_data(self, flags, label):
    #     self.edge_args_embedding_data_csv = pd.read_csv("filename.csv")
    #     self.edge_args_embedding_data_csv__cls[label].append({'class': self.mapping[label], 'data': flags_data_txt})


    def gen_node_name_embedding_data(self):
        # print('\t [gen_node_name_embedding_data] gen_node_name_embedding_data', node_name)
        print('[gen_node_name_embedding_data] self.node_names_by_graph', len(self.node_names_by_graph))
        # print('[gen_node_name_embedding_data] self.node_names_by_graph', self.node_names_by_graph)
        for graph_name in self.node_names_by_graph:
            label = graph_name.split('__')[0]
            nodes_names = ' '.join(self.node_names_by_graph[graph_name])
            if self.has_label is True:
                self.node_name_embedding_data_csv.append({'class': self.mapping[label], 'data': nodes_names, 'file': graph_name})
                # self.node_name_embedding_data_csv__cls[label].append('{}'.format(nodes_names))
                # self.node_name_embedding_data_csv__cls[label].append({'class': self.mapping[label], 'data': nodes_names, 'file': graph_name})
            else:
                self.node_name_embedding_data_csv.append({'class': -1, 'data': nodes_names, 'file': graph_name})

    def add_node_name_embedding_data(self, name, label, graph_name):
        if len(name) > 0:
            if graph_name not in self.node_names_by_graph:
                self.node_names_by_graph[graph_name] = [name]
            else:
                self.node_names_by_graph[graph_name] += [name]


    def encode_report(self, behavior, report_name, report_folder):
        """
        Process the data extracted from the report and save to data.json.
        Encode node, edge and save to graphs_dict.
        --------------------------------------
        Get and save the nodes, meta-path
            - report_folder: category of report (graph label)
            - report_name: graph name (graph id)
        Encode node and edge and save to graphs_dict also
        """

        # to debug single file
        # if report_name != '8f5135eec4dcb808423209163bbd94025ec47f4cb1b20dcf75b1fd56773ac58f.json':
        #     return

        self.current_node_id_of_current_graph = -1
        self.current_edge_id_of_current_graph = -1
        graph_name = report_folder+'__'+report_name
        
        #####################
        # Get all the procs
        #####################
        # print(len(behavior['processes']))
        # print(behavior['processtree'])
        procs = behavior['processes']

        # print('=================================')
        # print(self.pid_to_node)
        # print('=================================\n')
        # print('procs', procs)

        for proc in procs:
            calls = proc['calls']
            
            # id \t proc_name \t proc_path_severity \t regkey_written_severity \t dll_loaded_severity \t connects_host_severity
            # proc_name = proc['process_name']
            proc_name = proc['process_path']
            proc_info = '{}|{}'.format(graph_name, proc_name)

            if len(calls) > 0:  # this process does have api calls
                ##############################
                # Now loop through all the api calls
                ##############################
                for api in calls:
                    cat = api['category']
                    

                    # if cat in self.allow_cat and api['api'].lower() in self.interesting_apis:
                        
                    # if api['api'].lower() in self.interesting_apis:
                    #     api_name = api['api'].lower()
                    # else:
                    #     api_name = 'other'
                    api_name = api['api'].lower()
                        
                    api_time = api['time']
                    api_info = '{}|{}'.format(graph_name, api_name)

                    # print(api)

                    if api_name in self.interesting_apis:

                        if bool(api['flags']) is True: # not empty
                            self.add_edge_args_embedding_data(api['flags'], report_folder, graph_name)

                        __proc_identifier__ = graph_name+'_proc_'+str(proc['pid'])
                        # create parent node (root proc) if not inserted yet
                        if __proc_identifier__ not in self.pid_to_node:
                            self.increase_node()
                            proc_data = {
                                    # 'name': proc_name.replace(' ', '^'),
                                    'name': 'proc{'+str(proc['pid'])+'}',
                                    'pid': proc['pid'],
                                    'type': 'proc',
                                    'id': self.current_node_id,
                                    'id_in_graph': self.current_node_id_of_current_graph,
                                    'graph': graph_name,
                                    'graph_label': report_folder
                            }
                            # self.json_data['nodes'].append(proc_data)
                            self.json_data['nodes'][proc_data['id']] = proc_data
                            self.add_node_name_embedding_data('proc', report_folder, graph_name)
                                
                            self.pid_to_node[graph_name+'_proc_'+str(proc['pid'])] = proc_data
                            
                        if cat == 'file': # process API type file
                            self.process_API_file(api, api_info, proc_data, graph_name, report_folder)

                        elif cat == 'process': # process API type process
                            self.process_API_process(api, api_info, proc_data, graph_name, report_folder)

                        elif cat == 'registry': # process API type registry
                            self.process_API_registry(api, api_info, proc_data, graph_name, report_folder)
                        
                        else:
                            self.process_API_other(api, api_info, proc_data, graph_name, report_folder)
                
        # print('\tDone')
    
    def increase_node(self, api_info=None):
        self.current_node_id = self.current_node_id + 1
        self.current_node_id_of_current_graph = self.current_node_id_of_current_graph + 1

        self.api_nodes_existed_id[api_info] = self.current_node_id


    def process_API_other(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api'].lower()

        api_flags = None
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']

        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'].lower(),
                'type': 'other_api',
                        
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.add_node_name_embedding_data(api['api'].lower(), graph_label, graph_name)

        self.api_nodes_existed_id[api_info] = self.current_node_id

        # create edge from this api to proc node (parent_node)
        if self.reverse_edge is True:
            # print(graph_name, 'reverse_edge', node_api__data['name'], 'to', parent_node['name'])
            self.edge(node_api__data, parent_node, args={'api_flags': api_flags, 'edge_type': 'proc__other_api'}, graph_name=graph_name)
        else:
            # print(graph_name, node_api__data['name'], 'to', parent_node['name'])
            self.edge(parent_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__other_api'}, graph_name=graph_name)


    def process_API_process(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api'].lower()
        api_flags = None
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']
        # api_args = 'NULL'
        # if 'arguments' in api and api['arguments'] is not None and api['arguments'] != '':
        #     api_args = getInterestingArg(api['arguments'])


        
        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'].lower(),
                'type': 'process_api',
                
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.add_node_name_embedding_data(api['api'].lower(), graph_label, graph_name)

        self.api_nodes_existed_id[api_info] = self.current_node_id

        buffer_length = 0
        if 'buffer' in api['arguments'] and 'length' in api['arguments']:
            buffer_length = api['arguments']['length']

        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'process_handle' in api['arguments'] and 'process_identifier' in api['arguments'] and api['arguments']['process_identifier'] != 0:

            # create this process API node ONLY WHEN there is a reference from this API to the a process handle (process_handle != 0)
            # node process API data
            # self.increase_node()
            # node_api__data = {
            #     'name': api['api'].lower(),
            #     'type': 'process_api',
                    
            #     'id': self.current_node_id,
            #     'id_in_graph': self.current_node_id_of_current_graph,

            #     'graph': graph_name,
            #     'graph_label': graph_label
            # }
            # self.json_data['nodes'].append(node_api__data)
            # self.add_node_name_embedding_data(api['api'].lower(), graph_label, graph_name)
            # self.api_nodes_existed_id[api_info] = self.current_node_id


            process_identifier = str(api['arguments']['process_identifier'])
            # process_handle = api['arguments']['process_handle']

            # save this node to list pid_to_node first, in case later needs query
            # self.pid_to_node[graph_name+'_'+api['pid']] = node_api__data

            __identifier__ = graph_name+'_proc_'+process_identifier
            if __identifier__ not in self.pid_to_node:
                # if api_name == 'NtOpenProcess':
                # create a process node
                self.increase_node()
                # node file handle data
                node_process__data = {
                        'name': 'proc{'+str(process_identifier)+'}',
                        'type': 'proc',
                        
                        'id': self.current_node_id,
                        'id_in_graph': self.current_node_id_of_current_graph,

                        'graph': graph_name,
                        'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_process__data)
                self.json_data['nodes'][node_process__data['id']] = node_process__data
                self.add_node_name_embedding_data('proc', graph_label, graph_name)

                self.pid_to_node[__identifier__] = node_process__data
                    
            # get the node to connect to
            connect_node = self.pid_to_node[__identifier__]
                                    
            # create an edge from parent_node to node_api if the process_identifier is different from parent_node's pid
            if int(parent_node['pid']) != int(process_identifier):
                if self.reverse_edge is True:
                    self.edge(node_api__data, parent_node, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
                else:
                    self.edge(parent_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
            else:
                # create edge between this node and connect_node
                if 'open' in api_name or 'set' in api_name or 'write' in api_name or 'create' in api_name:
                    self.edge(node_api__data, connect_node, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
                else:
                    self.edge(connect_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
        
        # Actually we don't care about those API that do not reference process_identifier to any process_identifier, so just comment these
        else:
            # create edge from this api to proc node (parent_node)
            # but because this is process, this edge is the same with the edge created above (from node_api__data to connect_node)
            if self.reverse_edge is True:
                self.edge(node_api__data, parent_node, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
            else:
                self.edge(parent_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__process_api'}, graph_name=graph_name, buffer_size=buffer_length)
                

    def process_API_file(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api'].lower()

        api_flags = None
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']

        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'].lower(),
                'type': 'file_api',
                        
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.add_node_name_embedding_data(api['api'].lower(), graph_label, graph_name)

        self.api_nodes_existed_id[api_info] = self.current_node_id

        # if this api has file_handle, then get the file_handle, then find the node correspond with this file_handle, then connect the api_node with the file_handle node
        if 'file_handle' in api['arguments']:
            file_handle = api['arguments']['file_handle']

            __identifier__ = graph_name+'_file_'+file_handle
            # if api_name == 'NtCreateFile':
            if __identifier__ not in self.handle_to_node:
                # create a file node
                self.increase_node()
                # node file handle data
                node_file__data = {
                    'name': 'file{'+file_handle+'}',
                    'type': 'file',
                    
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_file__data)
                self.json_data['nodes'][node_file__data['id']] = node_file__data
                self.add_node_name_embedding_data('file', graph_label, graph_name)

                self.handle_to_node[__identifier__] = node_file__data
            
            connect_node = self.handle_to_node[__identifier__]

            buffer_length = 0
            if 'buffer' in api['arguments'] and 'length' in api['arguments']:
                buffer_length = api['arguments']['length']

            # create edge from this api node to connect_node (file handle) (file node)
            if 'open' in api_name or 'set' in api_name or 'write' in api_name or 'create' in api_name:
                self.edge(node_api__data, connect_node, args={'edge_type': 'file__file_api'}, graph_name=graph_name, buffer_size=buffer_length)
            else:
                self.edge(connect_node, node_api__data, args={'edge_type': 'file__file_api'}, graph_name=graph_name, buffer_size=buffer_length)

        # create edge from this api to proc node (parent_node)
        if self.reverse_edge is True:
            # print(graph_name, 'reverse_edge', node_api__data['name'], 'to', parent_node['name'])
            self.edge(node_api__data, parent_node, args={'api_flags': api_flags, 'edge_type': 'proc__file_api'}, graph_name=graph_name)
        else:
            # print(graph_name, node_api__data['name'], 'to', parent_node['name'])
            self.edge(parent_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__file_api'}, graph_name=graph_name)



    def process_API_registry(self, api, api_info, parent_node, graph_name, graph_label):
        api_name = api['api'].lower()

        api_flags = None
        if 'flags' in api and api['flags'] is not None and api['flags'] != '':
            api_flags = api['flags']

        # Check if this api (with the same characteristics (use name only as characteristics)) is called.
        # If not called, create new node
        if api_info not in self.api_nodes_existed_id.keys():
            # create a process api node
            self.increase_node()
            node_api__data = {
                'name': api['api'].lower(),
                'type': 'reg_api',
                        
                'id': self.current_node_id,
                'id_in_graph': self.current_node_id_of_current_graph,

                'graph': graph_name,
                'graph_label': graph_label
            }
            self.api_nodes_existed[api_info] = node_api__data
        else:
            node_api__data = self.api_nodes_existed[api_info]
        
            # print('node_api__data', node_api__data)

        # self.json_data['nodes'].append(node_api__data)
        self.json_data['nodes'][node_api__data['id']] = node_api__data
        self.add_node_name_embedding_data(api['api'].lower(), graph_label, graph_name)

        self.api_nodes_existed_id[api_info] = self.current_node_id

        # if this api has key_handle, then get the key_handle, then find the node correspond with this key_handle, then connect the api_node with the key_handle node
        if 'key_handle' in api['arguments']:
            key_handle = api['arguments']['key_handle']

            __identifier__ = graph_name+'_reg_'+key_handle
            # if api_name == 'NtOpenKey':
            if __identifier__ not in self.handle_to_node:
                # create a registry key node
                self.increase_node()
                # node key handle data
                node_reg__data = {
                    'name': 'reg{'+key_handle+'}',
                    'type': 'reg',
                    
                    'id': self.current_node_id,
                    'id_in_graph': self.current_node_id_of_current_graph,

                    'graph': graph_name,
                    'graph_label': graph_label,
                }
                # self.json_data['nodes'].append(node_reg__data)
                self.json_data['nodes'][node_reg__data['id']] = node_reg__data
                self.add_node_name_embedding_data('reg', graph_label, graph_name)

                self.handle_to_node[__identifier__] = node_reg__data
            
            connect_node = self.handle_to_node[__identifier__]

            buffer_length = 0
            if 'buffer' in api['arguments'] and 'length' in api['arguments']:
                buffer_length = api['arguments']['length']

            # create edge from this api node to connect_node (file handle) (file node)
            if 'open' in api_name or 'set' in api_name or 'write' in api_name or 'create' in api_name:
                self.edge(node_api__data, connect_node, args={'edge_type': 'reg__reg_api'}, graph_name=graph_name, buffer_size=buffer_length)
            else:
                self.edge(connect_node, node_api__data, args={'edge_type': 'reg__reg_api'}, graph_name=graph_name, buffer_size=buffer_length)

        # create edge from this api to proc node (parent_node)
        if self.reverse_edge is True:
            self.edge(node_api__data, parent_node, args={'api_flags': api_flags, 'edge_type': 'proc__reg_api'}, graph_name=graph_name)
        else:
            self.edge(parent_node, node_api__data, args={'api_flags': api_flags, 'edge_type': 'proc__reg_api'}, graph_name=graph_name)



    def edge(self, s, d, args, graph_name, buffer_size=0):
        '''
        edge(s, d, args, graph_name, buffer_size=0)
        '''
        self.current_edge_id += 1
        self.current_edge_id_of_current_graph = self.current_edge_id_of_current_graph + 1

        # if buffer_size <= 0:
        #     return

        # print('edge() graph_name', graph_name, 'self.current_edge_id', self.current_edge_id, 'self.current_edge_id_of_current_graph', self.current_edge_id_of_current_graph)
        # print('self.json_data', self.json_data)

        if buffer_size > 0:
            print('[edge] buffer', buffer_size)

        path_data = {
            # 'type': self.path_type_code[args['edge_type']],
            'type': args['edge_type'],
            'args': {},
            'from': s['id'],
            'to': d['id'],
            
            'from_in_graph': s['id_in_graph'],
            'to_in_graph': d['id_in_graph'],
            'id_in_graph': self.current_edge_id_of_current_graph,
            
            'id': self.current_edge_id,
            'buffer_size': buffer_size,

            'graph': graph_name
        }

        # care only when this api (s and d) is not other (in interesting apis)
        if args is not None and 'api_flags' in args and self.nodename_to_viz(s['name']) != 'other' and self.nodename_to_viz(d['name']) != 'other':
            # print('args', args, "args['api_flags']", args['api_flags'])
            path_data['args'] = args['api_flags']
            
        # self.json_data[args['edge_type']].append(path_data)
        self.json_data[args['edge_type']][path_data['id']] = path_data
        # self.json_data['path'][path_data['id']] = path_data



    # def encode_node(self, node):
    def encode_node(self, node_id):
        """
        Encode node information to node attribute
        ----------------------------
            Calculate node attributes (init features)
            All nodes must have same features space.
        """
        node = self.json_data['nodes'][node_id]

        # =======================================
        # Encode node name using Word Embedding
        # =======================================
        # print('[encode_node]')

        # Use name of API to represent each node
        self.nodes_labels = self.nodes_labels + [node['name']]

        ###################
        # Create graph
        ###################
        # print('[encode_node] self.graphs_dict.keys()', self.graphs_dict.keys())
        if node['graph'] not in self.graphs_dict.keys():
            print("\t [encode_node] node['graph']", node['graph'])
            self.graphs_name_to_label[node['graph']] = node['graph_label']
            self.graphs_dict[node['graph']] = DGLGraph(multigraph=True)
            if self.do_draw:
                # self.graphs_viz[node['graph']] = Digraph(name=node['graph_label'], format='png')
                self.graphs_viz[node['graph']] = nx.MultiDiGraph()

        ###################
        # Get features
        ###################

        ndata = {}

        ''' GNN_NODE_TYPES_KEY '''
        node_type_encoded = indices_to_one_hot(
            self.node_type_code[node['type']], out_vec_size=len(self.node_type_code))
        nte_torch = torch.from_numpy(np.array([node_type_encoded])).type(torch.FloatTensor)
        # print('nte_torch', nte_torch)
        ndata[GNN_NODE_TYPES_KEY] = nte_torch

        ''' GNN_NODE_LABELS_KEY '''
        # name_transformed = self.node_embedder.transform(self.nodename_to_str(node['name']))
        name_transformed = indices_to_one_hot(
            self.node_name_code[self.nodename_to_str(node['name'])], out_vec_size=len(self.node_name_code))
        # print('self.node_name_code', self.node_name_code)
        cbow_node = torch.tensor(name_transformed).type(torch.FloatTensor)
        ndata[GNN_NODE_LABELS_KEY] = cbow_node.view(1, -1)

        ''' id_in_graph '''
        ndata['nid'] = torch.tensor([node['id_in_graph']])

        # print('ndata[GNN_NODE_LABELS_KEY]', ndata[GNN_NODE_LABELS_KEY])
        # print('ndata[GNN_NODE_TYPES_KEY]', ndata[GNN_NODE_TYPES_KEY])

        ''' add node with data to graph '''
        self.graphs_dict[node['graph']].add_nodes(1, data=ndata)

        # for visualize
        if self.do_draw:
            if node['type'] in ['proc', 'file', 'reg']:
                shape = 'ellipse'
            else:
                shape = 'box' # api node
            # self.graphs_viz[node['graph']].node('n{}'.format(node['id_in_graph']), self.nodename_to_viz(node['name']), color=self.node_color[self.node_type_code[node['type']]], shape=shape)
            self.graphs_viz[node['graph']].add_node('n{}'.format(node['id_in_graph']), label=self.nodename_to_viz(node['name']), color=self.node_color[self.node_type_code[node['type']]], shape=shape)

        # torch.save(ndata, )
    
    def encode_node_process(self):
        print('[encode_node_process]')
        results = ThreadPool(__THREADS_NUM__).map(self.encode_node, list(self.json_data['nodes'].keys()))

    def encode_edge_type_process(self, key):
        print('[encode_edge_type_process]', key)
        if key == 'proc__file_api':
            f = self.encode_edge__proc__file_api
        elif key == 'file__file_api':
            f = self.encode_edge__file__file_api
        elif key == 'proc__reg_api':
            f = self.encode_edge__proc__reg_api
        elif key == 'reg__reg_api':
            f = self.encode_edge__reg__reg_api
        elif key == 'proc__process_api':
            f = self.encode_edge__proc__process_api
        elif key == 'proc__other_api':
            f = self.encode_edge__proc__other_api
        results = ThreadPool(__THREADS_NUM__).map(f, list(self.json_data[key].keys()))

    def encode_edge__proc__file_api(self, path_id):
        path = self.json_data['proc__file_api'][path_id]
        self.encode_edge(path)
    def encode_edge__file__file_api(self, path_id):
        path = self.json_data['file__file_api'][path_id]
        self.encode_edge(path)
    def encode_edge__proc__reg_api(self, path_id):
        path = self.json_data['proc__reg_api'][path_id]
        self.encode_edge(path)
    def encode_edge__reg__reg_api(self, path_id):
        path = self.json_data['reg__reg_api'][path_id]
        self.encode_edge(path)
    def encode_edge__proc__process_api(self, path_id):
        path = self.json_data['proc__process_api'][path_id]
        self.encode_edge(path)
    def encode_edge__proc__other_api(self, path_id):
        path = self.json_data['proc__other_api'][path_id]
        self.encode_edge(path)


    def encode_edge(self, path):
        """
        Encode edge information to node attribute
        """

        if len(path) <= 0:
            del self.graphs_name_to_label[path['graph']]
            del self.graphs_dict[path['graph']]
            if self.do_draw:
                del self.graphs_viz[path['graph']]
            return

        self.edges_labels.append(self.path_type_code[path['type']])
        # self.edges_labels.append(path['type_code'])

        edata = {}

        ''' GNN_EDGE_TYPES_KEY '''
        edge_type_encoded = indices_to_one_hot(
            self.path_type_code[path['type']], out_vec_size=len(self.path_type_code))
        ete_torch = torch.from_numpy(
            np.array([edge_type_encoded])).type(torch.FloatTensor)
        # print('ete_torch', ete_torch)
        edata[GNN_EDGE_TYPES_KEY] = ete_torch

        ''' GNN_EDGE_LABELS_KEY '''
        # cbow_edge = self.cbow_encode_edge_args(self.args_to_strargs_to_str(path['args']))
        # args_transformed, txt_chosen = self.edge_embedder.transform(self.args_to_str(path['args']))
        # print('path', path, '|', path['graph'])
        # print('\t Transform edge:', self.args_to_str(path['args']))
        args_transformed = self.edge_embedder.transform(self.args_to_str(path['args']))
        # print('\t args_transformed:', args_transformed)
        cbow_edge = torch.tensor(args_transformed).type(torch.FloatTensor)
        edata[GNN_EDGE_LABELS_KEY] = cbow_edge.view(1, -1)
        
        ''' eid '''
        edata['from_in_graph'] = torch.tensor([path['from_in_graph']])
        edata['to_in_graph'] = torch.tensor([path['to_in_graph']])
        edata['eid'] = torch.tensor([path['id_in_graph']])

        # print("\nself.args_to_str(path['args'])", txt_chosen, '||', self.args_to_str(path['args']))
        # print('edata[GNN_EDGE_TYPES_KEY]', edata[GNN_EDGE_TYPES_KEY])
        # print('edata[GNN_EDGE_LABELS_KEY]', edata[GNN_EDGE_LABELS_KEY])
        # print('edata[GNN_EDGE_LABELS_KEY].shape', edata[GNN_EDGE_LABELS_KEY].shape)

        ''' GNN_EDGE_BUFFER_SIZE_KEY '''
        edata[GNN_EDGE_BUFFER_SIZE_KEY] = torch.Tensor([[path['buffer_size']]])
        # print('edata[GNN_EDGE_LABELS_KEY]', edata[GNN_EDGE_LABELS_KEY])
        # print('edata[GNN_EDGE_BUFFER_SIZE_KEY]', edata[GNN_EDGE_BUFFER_SIZE_KEY])
        # print(path['buffer_size'])
            

        ''' add edge with data to graph '''
        # print("path['graph']", path['graph'])
        # print(self.graphs_dict[path['graph']].number_of_nodes())
        self.graphs_dict[path['graph']].add_edge(path['from_in_graph'], path['to_in_graph'], data=edata)

        if self.do_draw:
            if len(self.args_to_str(path['args'])) > 0:
                # self.graphs_viz[path['graph']].edge('n{}'.format(path['from_in_graph']), 'n{}'.format(path['to_in_graph']), label='{}'.format(self.args_to_str(path['args'])))
                self.graphs_viz[path['graph']].add_edge('n{}'.format(path['from_in_graph']), 'n{}'.format(path['to_in_graph']), label='{}'.format(self.args_to_str(path['args'])), eid=path['id_in_graph'])
            else:
                # self.graphs_viz[path['graph']].edge('n{}'.format(path['from_in_graph']), 'n{}'.format(path['to_in_graph']))
                self.graphs_viz[path['graph']].add_edge('n{}'.format(path['from_in_graph']), 'n{}'.format(path['to_in_graph']), eid=path['id_in_graph'])


    def gen_vocab_and_corpus(self):
        print('[gen_vocab_and_corpus] self.vocab_path', self.vocab_path, self.vocab_path_node, self.vocab_path_edge)
        # first create dictionary
        if not os.path.isdir(self.vocab_path):
            os.makedirs(self.vocab_path)
        
        if not os.path.exists(self.vocab_path_node):
            self.word_dict_node.append('other')
            self.prepend_vocab = True
        else:
            # read from dict node
            with open(self.vocab_path_node, 'r') as f:
                vocab = f.read().strip()
                self.word_dict_node = vocab.split(' ')

        if not os.path.exists(self.vocab_path_edge):
            self.word_dict_edge.append('null')
            self.prepend_vocab = True
        else:
            # read from dict edge
            with open(self.vocab_path_edge, 'r') as f:
                vocab = f.read().strip()
                self.word_dict_edge = vocab.split(' ')


    def train_and_load_embedding(self):
        ''' Save all corpus to transform '''
        if self.prepare_word_embedding:
            # print('[train_and_load_embedding] prepare_word_embedding, self.edge_args_embedding_data_csv', self.edge_args_embedding_data_csv)
            if self.train_embedder:
                # self.edge_embedder.prepare(self.edge_args_embedding_data_csv, self.edge_args_embedding_data_csv__cls, self.train_list_name, self.test_list_name, self.flags_keys)
                # self.node_embedder.prepare(self.node_name_embedding_data_csv, self.node_name_embedding_data_csv__cls, self.train_list_name, self.test_list_name)
                self.edge_embedder.prepare(self.edge_args_embedding_data_csv, self.train_list_name, self.test_list_name, self.flags_keys)
                self.node_embedder.prepare(self.node_name_embedding_data_csv, self.train_list_name, self.test_list_name)

            self.edge_embedder.save_corpus(self.edge_args_embedding_data_csv)
            self.node_embedder.save_corpus(self.node_name_embedding_data_csv)
        else:
            # Copy corpus file from emb_trained_path
            if not os.path.exists(self.edge_args_emb_corpus_path+'/corpus.csv'):
                shutil.copy(self.edge_args_emb_trained_path+'/corpus.csv', self.edge_args_emb_corpus_path+'/corpus.csv')
                shutil.copy(self.node_name_emb_trained_path+'/corpus.csv', self.node_name_emb_corpus_path+'/corpus.csv')


        if self.train_embedder:
            ''' Train embedding for edge arguments and node names '''
            self.word_to_ix_edge, word_dict_edge = self.edge_embedder.train(preprocess_level=self.preprocess_level)
            self.word_to_ix_node, word_dict_node = self.node_embedder.train(preprocess_level=self.preprocess_level)
            ''' Load on this whole corpus '''
            self.edge_embedder.load(load_train_test_set=False, preprocess_level=self.preprocess_level)
            self.node_embedder.load(load_train_test_set=False, preprocess_level=self.preprocess_level)
        else:
            ''' Load tf-idf '''
            print('\n---------------- [train_and_load_embedding] Load vectorizer')
            self.word_to_ix_edge, word_dict_edge = self.edge_embedder.load(preprocess_level=self.preprocess_level)
            self.word_to_ix_node, word_dict_node = self.node_embedder.load(preprocess_level=self.preprocess_level)

        for w in word_dict_node:
            # print('w', w)
            if w not in self.word_dict_node:
                self.word_dict_node.append(w)
        for w in word_dict_edge:
            if w not in self.word_dict_edge:
                self.word_dict_edge.append(w)

        if 'other' not in self.word_dict_node:
            self.word_dict_node = ['other'] + self.word_dict_node
        if 'null' not in self.word_dict_edge:
            self.word_dict_edge = ['null'] + self.word_dict_edge


        ''' save vocab '''
        if self.prepend_vocab is True:
            print('[train_and_load_embedding] save vocab')
            with open(self.vocab_path_node, 'w') as f:
                f.write(' '.join(self.word_dict_node))
        if self.prepend_vocab is True:
            print('[train_and_load_embedding] save vocab')
            with open(self.vocab_path_edge, 'w') as f:
                f.write(' '.join(self.word_dict_edge))


    def encode_data(self):
        """
        Encode nodes & edges from data.json
        """

        if 'nodes' in self.json_data.keys():
            n_num = 0
            n_tot = len(self.json_data['nodes'])
            # self.embed_nodes = nn.Embedding(n_tot, 1)
            for node_id in self.json_data['nodes']:
                # node = self.json_data['nodes'][node_id]
                self.encode_node(node_id)
                n_num += 1
                if n_num % 100000 == 0 or n_num == n_tot:
                    print('\t [encode_data] {}/{}'.format(n_num, n_tot))
        
        for key in self.json_data:
            if key != 'nodes':
                p_num = 0
                p_tot = len(self.json_data[key])
                # self.embed_edges = nn.Embedding(p_tot, 1)
                print('[encode_data] encode_edge type '+key)
                # print(self.json_data[key], key)
                for path_id in self.json_data[key]:
                    path = self.json_data[key][path_id]
                    # print('encode_edge ', path)
                    self.encode_edge(path)
                    p_num += 1
                    if p_num % 100000 == 0 or p_num == p_tot:
                        print('\t [encode_data] {}/{}'.format(p_num, p_tot))

        # pn = Process(target=self.encode_node_process)
        # pn.start()
        # pn.join()
        # self.encode_node_process()
        
        # if 'nodes' in self.json_data.keys():
        #     with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
        #         for node_id in self.json_data['nodes']:
        #             node = self.json_data['nodes'][node_id]
        #             executor.submit(self.encode_node, node)

        # print('[encode_data] self.graphs_dict', self.graphs_dict.keys())

        # for key in self.json_data:
        #     if key != 'nodes':
        #         # pe = Process(target=self.encode_edge_type_process, args=(key,))
        #         # pe.start()
        #         # pe.join()

        #         # with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
        #         #     for path_id in self.json_data[key]:
        #         #         path = self.json_data[key][path_id]
        #         #         executor.submit(self.encode_edge, path)

        #         p_num = 0
        #         p_tot = len(self.json_data[key])
        #         # self.embed_edges = nn.Embedding(p_tot, 1)
        #         print('[encode_data] encode_edge type '+key)
        #         # print(self.json_data[key], key)
        #         for path_id in self.json_data[key]:
        #             path = self.json_data[key][path_id]
        #             # print('encode_edge ', path)
        #             self.encode_edge(path)
        #             p_num += 1
        #             if p_num % 100000 == 0 or p_num == p_tot:
        #                 print('\t [encode_data] {}/{}'.format(p_num, p_tot))

        # with open(self.pickle_folder+'/all_nodes.txt', 'w') as f:
        #     f.write('\n'.join(self.all_nodes))



    def create_graph_from_report_multithread(self, report_file_path):
        report_dir_name = os.path.basename(os.path.dirname(report_file_path))
        report_file_name = os.path.basename(report_file_path)
        g_name = '{}__{}'.format(report_dir_name, report_file_name)
        self.create_graph_from_report(g_name, report_dir_name)



    def create_graph_from_report(self, g_name, g_label):
        if os.path.exists(self.graph_folder+'/'+g_name.split('.')[0]):
            print('[create_graph_from_report]', g_name+' encoded ('+self.graph_folder+'/'+g_name.split('.')[0]+'). Skip')
            return
        print('[create_graph_from_report] encode', g_name)

        if 'nodes' in self.json_data.keys():
            n_num = 0
            n_tot = len(self.json_data['nodes'])
            # self.embed_nodes = nn.Embedding(n_tot, 1)

            # nodes = []
            # for node_id in self.json_data['nodes']:
            #     node = self.json_data['nodes'][node_id]
            #     if node['graph'] != g_name:
            #         continue
            #     else:
            #         nodes.append(node)
            # results = ThreadPool(__THREADS_NUM__).map(self.encode_node, nodes)
            with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
                for node_id in self.json_data['nodes']:
                    node = self.json_data['nodes'][node_id]
                    if node['graph'] != g_name:
                        continue
                    else:
                        executor.submit(self.encode_node, node)
            
            # for node_id in self.json_data['nodes']:
            #     node = self.json_data['nodes'][node_id]
            #     if node['graph'] != g_name:
            #         continue
            #     else:
            #         # print('\t [create_graph_from_report] node graph', node['graph'], g_name)
            #         self.encode_node(node)
            #         n_num += 1
            #         if n_num % 100000 == 0 or n_num == n_tot:
            #             print('\t [create_graph_from_report] {}/{}'.format(n_num, n_tot))
        
        for key in self.json_data:
            if key != 'nodes':
                p_num = 0
                p_tot = len(self.json_data[key])
                # self.embed_edges = nn.Embedding(p_tot, 1)

                paths = []
                for path_id in self.json_data[key]:
                    path = self.json_data[key][path_id]
                    if path['graph'] != g_name:
                        continue
                    else:
                        paths.append(path)
                results = ThreadPool(__THREADS_NUM__).map(self.encode_edge, paths)

                # with ThreadPoolExecutor(max_workers=__THREADS_NUM__) as executor:
                #     for path_id in self.json_data[key]:
                #         path = self.json_data[key][path_id]
                #         if path['graph'] != g_name:
                #             continue
                #         else:
                #             executor.submit(self.encode_edge, path)

                # for path_id in self.json_data[key]:
                #     path = self.json_data[key][path_id]
                #     if path['graph'] != g_name:
                #         continue
                #     else:
                #         # print('\t edge', path)
                #         self.encode_edge(path)
                #         p_num += 1
                #         if p_num % 100000 == 0 or p_num == p_tot:
                #             print('\t [create_graph_from_report] {}/{}'.format(p_num, p_tot))

        if g_name not in self.graphs_name_to_label:
            print('\t [create_graph_from_report]', g_name, 'do not have any nodes. Skip')
            return

        # g_label = self.graphs_name_to_label[g_name]
        graph = self.graphs_dict[g_name]

        if not graph.edata:
            del self.graphs_name_to_label[g_name]
            del self.graphs_dict[g_name]

        print('[create_graph_from_report] * Save graph to', os.path.join(self.graph_folder, g_label, g_name.split('.')[0]))
        save_pickle(graph, os.path.join(self.graph_folder, g_label, g_name.split('.')[0]))

        if self.do_draw:
            # gdot_path = '{}/{}/{}'.format(self.graph_viz_dir, os.path.basename(self.reports_parent_dir_path), g_name)
            gdot_path = '{}/{}'.format(self.graph_viz_dir, g_name)
            print('[create_graph_from_report] ** Output graphviz', gdot_path)
            nx.drawing.nx_pydot.write_dot(self.graphs_viz[g_name], gdot_path)
            # self.graphs_viz[g_name].render(filename=gdot_path)
            (graph,) = pydot.graph_from_dot_file(gdot_path)
            print('[create_graphs] write_png')
            graph.write_png('{}/{}.png'.format(self.graph_viz_dir, g_name))
        return g_name

    
    def merge_graphs_extension(self, dirpath):
        k = 0
        for g_file in os.listdir(dirpath):
            k += 1
            g_label = g_file.split('__')[0]
            g_name = g_file+'.json'
            print('[merge_graphs]', k, 'graph path', os.path.join(dirpath, g_file))
            graph = load_pickle(os.path.join(dirpath, g_file))

            n_nodes = graph.number_of_nodes()
            if n_nodes > self.max_n_nodes:
                self.max_n_nodes = n_nodes

            self.graphs.append(graph)
            self.graphs_names.append(g_name)
            self.graphs_labels.append(g_label)

    def merge_graphs(self):
        ##############################
        # Append to graphs list
        ##############################
        print('self.graph_folder', self.graph_folder)
        if self.has_label is True:
            for lbl in os.listdir(self.graph_folder):
                if not os.path.isdir(self.graph_folder+'/'+lbl):
                    continue
                self.merge_graphs_extension(dirpath=self.graph_folder+'/'+lbl)
        else:
                self.merge_graphs_extension(dirpath=self.graph_folder)

        self.save_pickle_data()
        num_entities = len(set(self.nodes_labels))
        num_rels = len(set(self.edges_labels))

        # Save additional data
        save_pickle(num_entities, os.path.join(self.pickle_folder, N_ENTITIES))
        save_pickle(num_rels, os.path.join(self.pickle_folder, N_RELS))
        save_pickle(self.max_n_nodes, os.path.join(self.pickle_folder, MAX_N_NODES))

        self.data_dortmund_format[N_ENTITIES] = num_entities
        self.data_dortmund_format[N_RELS] = num_rels
        self.data_dortmund_format[MAX_N_NODES] = self.max_n_nodes


    def create_graphs(self):
        """
        Create graphs from encoded data to feed to network
        """
        # print(self.graphs_name_to_label)
        print('[create_graphs] len(self.graphs_dict)', len(self.graphs_dict))
        print('[create_graphs] self.graphs_dict.keys()', self.graphs_dict.keys())
        ##############################
        # Append to graphs list
        ##############################
        gnum = 0
        for g_name in list(self.graphs_name_to_label.keys()):
            g_label = self.graphs_name_to_label[g_name]
            graph = self.graphs_dict[g_name]

            if not graph.edata:
                del self.graphs_name_to_label[g_name]
                del self.graphs_dict[g_name]

            else:
                n_nodes = graph.number_of_nodes()
                if n_nodes > self.max_n_nodes:
                    self.max_n_nodes = n_nodes

                ######################
                # Normalize edge
                ######################
                # edge_src, edge_dst = graph.edges()

                # edge_dst = list(edge_dst.data.numpy())
                # print('[create_graphs] graph.edata ('+g_label+')', graph.edata)
                # # edge_type = list(graph.edata[GNN_EDGE_TYPES_KEY])
                # edge_lbl = list(graph.edata[GNN_EDGE_LABELS_KEY])

                # # print('[create_graphs] edge_dst, edge_type', edge_dst, edge_type)
                # # _, inverse_index, count = np.unique((edge_dst, edge_type), axis=1, return_inverse=True, return_counts=True)
                # _, inverse_index, count = np.unique((edge_dst, edge_lbl), axis=1, return_inverse=True, return_counts=True)
                # degrees = count[inverse_index]
                # edge_norm = np.ones(
                #     len(edge_dst), dtype=np.float32) / degrees.astype(np.float32)
                # graph.edata[GNN_EDGE_NORM] = torch.FloatTensor(edge_norm)

                self.graphs.append(graph)
                self.graphs_names.append(g_name)
                self.graphs_labels.append(g_label)

                # Save this graph to png
                # if gnum < 10:
                # json_file_size = os.path.getsize(self.reports_parent_dir_path+'/'+g_label+'/'+g_name.split('__')[1])
                # if json_file_size // 1000000 <= 200: # 250000000
                if self.do_draw:
                    # print(graph)
                    # nx.draw(graph.to_networkx(), with_labels=True)
                    # plt.savefig('data/graphs/{}.png'.format(g_name))
                    # print(self.graphs_viz[g_name].source)

                    # gdot_path = '{}/{}/{}'.format(self.graph_viz_dir, os.path.basename(self.reports_parent_dir_path), g_name)
                    gdot_path = '{}/{}'.format(self.graph_viz_dir, g_name)
                    print('[create_graphs] ** Output graphviz', gdot_path)
                    # self.graphs_viz[g_name].render(filename=gdot_path)
                    nx.drawing.nx_pydot.write_dot(self.graphs_viz[g_name], gdot_path)
                    (graph,) = pydot.graph_from_dot_file(gdot_path)
                    print('[create_graphs] write_png')
                    graph.write_png('{}/{}/{}.png'.format(self.graph_viz_dir, os.path.basename(self.reports_parent_dir_path), g_name))

                gnum += 1

        # print(self.graphs)

        self.save_pickle_data()

        num_entities = len(set(self.nodes_labels))
        num_rels = len(set(self.edges_labels))

        # Save additional data
        save_pickle(num_entities, os.path.join(self.pickle_folder, N_ENTITIES))
        save_pickle(num_rels, os.path.join(self.pickle_folder, N_RELS))
        save_pickle(self.max_n_nodes, os.path.join(self.pickle_folder, MAX_N_NODES))

        self.data_dortmund_format[N_ENTITIES] = num_entities
        self.data_dortmund_format[N_RELS] = num_rels
        self.data_dortmund_format[MAX_N_NODES] = self.max_n_nodes


    def save_pickle_data(self):
        print('[save_pickle_data] self.graphs', len(self.graphs))
        save_pickle(self.graphs, os.path.join(self.pickle_folder, GRAPH))
        save_pickle(self.graphs_names, os.path.join(self.pickle_folder, GNAMES))
        save_txt(self.graphs_names, os.path.join(self.pickle_folder, GNAMES+'.txt'))

        self.data_dortmund_format = {
            GRAPH: self.graphs,
            GNAMES: self.graphs_names,
        }

        if self.has_label is True:
            if self.mapping_path is None:
                label_set = set(sorted(self.graphs_labels))  # malware: 0, benign: 1
                num_labels = len(label_set)
                self.mapping = dict(zip(label_set, list(range(num_labels))))
                if self.pickle_folder is not None:
                    with open(self.pickle_folder+'/mapping.json', 'w') as f:
                        json.dump(self.mapping, f)
            
            # mapping = self.mapping_labels
            num_labels = len(self.mapping)
            print('[save_pickle_data] num_labels', num_labels)
            print('[save_pickle_data] mapping', self.mapping)
            labels = [self.mapping[label] for label in self.graphs_labels]
            # print('[save_pickle_data] labels', labels)
            # print('[save_pickle_data] label_set', label_set)

            labels_torch = torch.LongTensor(labels)
            print('[save_pickle_data] labels_torch', labels_torch)

            torch.save(labels_torch, os.path.join(self.pickle_folder, LABELS))
            save_pickle(num_labels, os.path.join(self.pickle_folder, N_CLASSES))
            save_pickle(self.graphs_labels, os.path.join(self.pickle_folder, LABELS_TXT))

            self.data_dortmund_format[N_CLASSES] = num_labels
            self.data_dortmund_format[LABELS] = labels_torch
            self.data_dortmund_format[LABELS_TXT] = self.graphs_labels


    def load_from_pickle(self):
        """
        Load data from pickle files
        """
        print('[load_from_pickle]', os.path.join(self.pickle_folder, GRAPH))
        print('[load_from_pickle]', GNAMES, os.path.join(self.pickle_folder, GNAMES))
        self.data_dortmund_format = {
            GRAPH: load_pickle(os.path.join(self.pickle_folder, GRAPH)),
            GNAMES: load_pickle(os.path.join(self.pickle_folder, GNAMES)),
            N_ENTITIES: load_pickle(os.path.join(self.pickle_folder, N_ENTITIES)),
            N_RELS: load_pickle(os.path.join(self.pickle_folder, N_RELS)),
            MAX_N_NODES: load_pickle(os.path.join(self.pickle_folder, MAX_N_NODES)),
            # GRAPH_ADJ: torch.load(os.path.join(self.pickle_folder, GRAPH_ADJ))
        }
        if self.has_label is True:
            self.data_dortmund_format[N_CLASSES] = load_pickle(os.path.join(self.pickle_folder, N_CLASSES))
            self.data_dortmund_format[LABELS] = torch.load(os.path.join(self.pickle_folder, LABELS))
            self.data_dortmund_format[LABELS_TXT] = load_pickle(os.path.join(self.pickle_folder, LABELS_TXT))

        return self.data_dortmund_format

    def cbow_encode_node_name(self, raw_text):
        data = []
        # target = raw_text
        data.append(raw_text)
        return make_vector(data, self.word_to_ix_node)
        # return make_vector(data, self.word_to_ix_node)


    # def cbow_encode_edge_args(self, raw_text):
    #     data = []
    #     data.append(raw_text)
    #     return make_vector(data, self.word_to_ix_edge)

    def cbow_encode_edge_args(self, raw_text):
        if len(raw_text) == 0 or raw_text == 'null':
            raw_text = 'null null'

        raw_text = raw_text.split(' ')
        # print('\t [cbow_encode_edge_args] raw_text', raw_text)
        if len(raw_text) == 1:
            # raw_text.append('null')
            raw_text = ['null', raw_text[0]]
        
        data = []
        i = 1
        while i < len(raw_text):
            target = raw_text[i]
            context = [raw_text[i - 1], target]
            data.append((context, target))
            i += 2
        # print('[cbow_encode_edge_args] data', data)
        return make_context_vector(data[0][0], self.word_to_ix_edge)


    def nodename_to_viz(self, txt):
        txt = txt.lower().strip()

        # print('self.use_interesting_apis', self.use_interesting_apis)
        if self.use_interesting_apis is False:
            return txt
        
        if txt.split('{')[0] not in self.interesting_apis:
            txt = 'other{'+txt+'}'
        return txt

    def nodename_to_str(self, txt):
        txt = txt.lower().strip()

        self.all_nodes.append(txt)

        # print('\nnode_name', txt)
        if self.use_interesting_apis is False:
            return txt.split('{')[0]

        if txt.split('{')[0] not in self.interesting_apis:
            # print('Not in interesting_apis. Convert from {} to other'.format(txt))
            return 'other'

        return txt.split('{')[0]

    def args_to_str(self, args_):
        # get flags values only. ignore keys
        # print('args_', args_)
        arr_val = []
        for key in sorted(args_):
            values_txt = key + ' ' + args_[key].replace('|', ' ')
            # values_txt = args_[key].replace('|', ' ')
            arr_val += values_txt.split(' ')
        arr_val = [v.strip().lower() for v in arr_val]
        # if len(arr_val) == 0:
        #     arr_val = ["null"] * 4
        str_val = ' '.join(arr_val)
        # print('str_val', str_val)
        return str_val

        # str_ = str(args_).lower()
        # str_ = str_.replace('{', '').replace('}', '').replace('\'', '').replace(
        #     '"', '').replace(':', ' ').replace(',', ' ').replace('|', ' ').replace('  ', ' ')
        # return str_




class CBOW(nn.Module):

    def __init__(self):
        pass

    def forward(self, inputs):
        pass


def make_vector(words, word_to_ix):
    idxs = [word_to_ix[w] for w in words]
    return torch.tensor(idxs)
    # return torch.tensor([idxs])


def make_context_vector(context, word_to_ix):
    # print('context', context)
    idxs = []
    for w in context:
        if len(w) == 0:
            v = 0.0
        else:
            w = w.lower()
            v = word_to_ix[w] if w in word_to_ix else 0.0
        idxs.append(v)
    # idxs = [word_to_ix[w] for w in context]
    return torch.tensor(idxs)
    # return torch.tensor([idxs])


